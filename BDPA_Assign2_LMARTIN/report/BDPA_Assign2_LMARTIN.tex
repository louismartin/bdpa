\documentclass[a4paper,10pt]{article}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}       % include graphics
\usepackage{subcaption}     % subfigures
\usepackage{float}          % placement of floats
\usepackage{fancyhdr}       % head notes and foot notes
\usepackage{bbm}            % Nice symbols
\usepackage{mathtools}      % Math tools like operators
\usepackage{listings}       % Write code in a listing
\usepackage[left=2cm,right=2cm,top=3cm,bottom=3cm]{geometry}

\graphicspath{ {assets/} }

% Operators
\DeclarePairedDelimiter\abs{\lvert}{\rvert} % abs
\DeclarePairedDelimiter\norm{\lVert}{\rVert} % norm
\DeclareMathOperator*{\argmax}{arg\,max} % argmax
\newcommand{\p}{\mathbbm{P}} % Big P for probabilties
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}  % Partial derivative

% To prevent the tilde from being printing above with lstlisting
\lstset{
    literate={~} {$\sim$}{1},
    showstringspaces=false,
    numbers=left,
    breaklines=true
}

% Head and foot notes
\pagestyle{fancy}
\fancyhf{}
\lhead{Louis MARTIN}
\rhead{Big Data Processing and Analytics: Assignment 2}
\rfoot{Page \thepage}


\title{Big Data Processing and Analytics: Assignment 2}
\author{Louis MARTIN\\
\href{mailto:louis.martin@student.ecp.fr}{\tt louis.martin@student.ecp.fr}}


\begin{document}
\maketitle

In this assignment we are going to compare documents using the Jaccard similarity.
For the sake of simplicity we are going to run our algorithms on the works of
William Shakespeare, each line being considered as a document.

\section{Setup}
This section is the same as for the first assignment.
\subsection{System specifications}

\begin{itemize}
    \item \textbf{Operating system}:\\
    Ubuntu 16.04 (Native)
    \item \textbf{System specifications}:\\
    Model: Dell Inspiron 17R 5720\\
    Processor: i5-3210M\\
    Cores: 2\\
    Threads: 4\\
    Ram: 8 GB\\
    Storage: 256GB SSD (MLC)
    \item \textbf{Java version}:\\
    openjdk version "1.8.0\_121"\\
    OpenJDK Runtime Environment (build 1.8.0\_121-8u121-b13-0ubuntu1.16.04.2-b13)\\
    OpenJDK 64-Bit Server VM (build 25.121-b13, mixed mode)
    \item \textbf{Haddop version}:\\
    Hadoop 2.7.3\\
    Subversion https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff\\
    Compiled by root on 2016-08-18T01:41Z\\
    Compiled with protoc 2.5.0\\
    From source with checksum 2e4ce5f957ea4db193bce3734ff29ff4\\
    This command was run using /usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.3.jar\\

\end{itemize}

Hadoop was installed using \href{https://www.digitalocean.com/community/tutorials/how-to-install-hadoop-in-stand-alone-mode-on-ubuntu-16-04}{this tutorial} and configured using \href{https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html}{this tutorial}.


\subsection{Configuration}
The configuration comes from the \href{https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html}{official documentation} for a single Hadoop node cluster.
The following configuration files allows Hadoop and YARN to run in a pseudo-distributed mode.
\begin{itemize}
    \item \textbf{core-site.xml:}
    \lstinputlisting[firstline=19, lastline=24, language=xml]{assets/hadoop_conf/core-site.xml}
    \item \textbf{hdfs-site.xml:}
    \lstinputlisting[firstline=19, lastline=24, language=xml]{assets/hadoop_conf/hdfs-site.xml}
    \item \textbf{mapred-site.xml:}
    \lstinputlisting[firstline=19, lastline=24, language=xml]{assets/hadoop_conf/mapred-site.xml}
    \item \textbf{yarn-site.xml:}
    \lstinputlisting[firstline=15, lastline=29, language=xml]{assets/hadoop_conf/yarn-site.xml}
    \item \textbf{Commands to set up HDFS and YARN:}
    \begin{lstlisting}[language=bash]
      # Format the filesystem
      hdfs namenode -format
      # Start HDFS
      start-dfs.sh

      # Create directories to execute MapReduce jobs
      hdfs dfs -mkdir /user
      hdfs dfs -mkdir /user/louis

      # Put the data in HDFS
      hdfs dfs -put ~/dev/bdpa/a1/data data

      # Start YARN Ressource manager
      start-yarn.sh
    \end{lstlisting}
\end{itemize}

\subsection{Workflow}
As I ran everything from the command line interface, I created helper shell scripts
in order to facilitate the compilation and execution of the mapreduce jobs.
\begin{itemize}

\item \textbf{compile.sh}:\\
The $compile.sh$ script which takes as input parameter the name of the main
class of the $.java$ file to compile it into a $.jar$ file.
\\Usage: \lstinline{./compile.sh Preprocess}.
\lstinputlisting[language=bash]{assets/code/compile.sh}
\item \textbf{run.sh}:\\
The compilation and execution of the map-reduce job is then wrapped into this script.
It takes as input the main class and the input file or directory.
\\Usage: \lstinline{./run.sh Preprocess data/corpus}.
\lstinputlisting[language=bash]{assets/code/run.sh}


\end{itemize}

\section{Pre-processing the input (10)}

The following preprocessing is done with a mapreduce job in \textbf{Preprocess.java}.
Each line is read by a single map call, processed and the written to a file.
The key we used to uniquely identify a line is its byte offset, which is natively
provided in the map call.
The value is the processed line.

\subsection{(2) Remove all stopwords (you can use the stopwords file of your previous
assignment), special characters (keep only [a-z],[A-Z] and [0-9]) and keep each unique
word only once per line. Don’t keep empty lines.}
\subsubsection{WordCount.java}
In order to remove the stopwords we get the number of occurences of each word in
the corpus and store it in wordcount.csv.
We will reuse these wordcounts for sorting the words.
\begin{itemize}
  \item \textbf{WordCount.java} (inspired from the official documentation and the first assignment):
  \lstinputlisting[firstnumber=18, firstline=18, lastline=51, language=java]{assets/code/WordCount.java}
  \item Output \textbf{wordcount.csv}:\\
  Here is an extract from the output
  \lstinputlisting[firstnumber=1010, firstline=1010, lastline=1020]{assets/code/wordcount.csv}
\end{itemize}

\subsubsection{Preprocess.java}
We then preprocess the lines using the wordcounts.
The preprocessing is done by a single map task (no reduce taks).
\lstinputlisting[firstnumber=124, firstline=124, lastline=126, language=java]{assets/code/Preprocess.java}
\begin{itemize}
  \item \textbf{Clean the line}:\\
  We first clean the lines from characters other than alpha-numerical characters.
  \lstinputlisting[firstnumber=77, firstline=77, lastline=79, language=java]{assets/code/Preprocess.java}

  \item \textbf{Keep each word once per line}:\\
  \lstinputlisting[firstnumber=80, firstline=80, lastline=81, language=java]{assets/code/Preprocess.java}

  \item \textbf{Remove all stopwords}:\\
  A stopword is defined by a word with less than 4000 occurences.
  The wordcounts are stored in a hashmap for O(1) element retrieval.
  \lstinputlisting[firstnumber=82, firstline=82, lastline=87, language=java]{assets/code/Preprocess.java}
\end{itemize}

\subsection{(1) Store on HDFS the number of output records (i.e., total lines).}
As we implemented a map only job, the number of output records is given by the Counter
\textbf{MAP\_OUTPUT\_RECORDS}. The number of output records is \textbf{114982}.
\lstinputlisting[firstnumber=139, firstline=139, lastline=150, language=java]{assets/code/Preprocess.java}

\subsection{(7) Order the tokens of each line in ascending order of global frequency.}
We need to sort the resulting words based on their respective count.
With this aim in mind, we implemented a custom comparator to sort the array of words
based on their counts that we previously stored in a HashMap:
\lstinputlisting[firstnumber=67, firstline=67, lastline=73, language=java]{assets/code/Preprocess.java}
We then sort the words using this comparator:
\lstinputlisting[firstnumber=91, firstline=91, lastline=91, language=java]{assets/code/Preprocess.java}




\section{Set-similarity joins (90)}
\subsection{(40) Perform all pair-wise comparisons between documents, using the following
technique: Each document is handled by a single mapper (remember that lines are
used to represent documents in this assignment). The map method should emit, for
each document, the document id along with one other document id as a key (one such
pair for each other document in the corpus) and the document’s content as a value.
In the reduce phase, perform the Jaccard computations for all/some selected pairs.
Output only similar pairs on HDFS, in TextOutputFormat.
Make sure that the same pair of documents is compared no more than once. Report
the execution time and the number of performed comparisons.}
\subsubsection{run\_naive\_similarity.sh}
We wrapped the execution of the necessary jobs in the \textbf{run\_naive\_similarity.sh}.

As the number of comparisons is simply too large for my computer to handle (too long
and to much storage taken), I reduced the number of lines to be compared to 5000.
\lstinputlisting{assets/code/run_naive_similarity.sh}

\subsubsection{NaiveSimilarity.java}
\begin{itemize}
  \item \textbf{Jaccard similarity}:\\
  The jaccard similarity between two documents is the length of their intersection,
  divided by their union.
  \lstinputlisting[firstnumber=64, firstline=64, lastline=73, language=java]{assets/code/NaiveSimilarity.java}

  \item \textbf{Mapper}:\\
  We first read all keys in an array.
  Then the Mapper reads each preprocessed lines. It finally outputs a pair of the
  key of the current line and another key for each other line in the corpus.
  The value is the content of the line.

  In order to retrieve the content of both documents, we need to make sure that
  they will end up in the same reducer and hence have the same key.
  We put the lowest key first for always having a single key.
  This also ensures that each pair is compared only once (more details in the comments):
  \lstinputlisting[firstnumber=75, firstline=75, lastline=105, language=java]{assets/code/NaiveSimilarity.java}

  \item \textbf{Reducer}:\\
  Each reduce call is supposed to receive exactly two values, one for each document.
  \lstinputlisting[firstnumber=107, firstline=107, lastline=129, language=java]{assets/code/NaiveSimilarity.java}
\end{itemize}

The number of comparison is simply the number of reduce calls given by the \textbf{REDUCE\_INPUT\_GROUPS} counter.
The number of comparison is \textbf{12497500} which is exactly $\frac{(n-1) n}{2}$ with $n=5000$ being the number of documents.
The first document is compared to $n-1$ documents, the second to $n-2$ documents.
It is therefore equal to $\sum_{1}^{n-1} = \frac{(n-1) n}{2}$ (not that the upper bound is $n-1$).

The execution time on 5000 documents is \textbf{131.62 s}

\subsection{(40) Create an inverted index, only for the first $|d| - ceil(t \times |d|) + 1$ words of each
document d (remember that they are stored in ascending order of frequency). In your
reducer, compute the similarity of the document pairs. Output only similar pairs on
HDFS, in TextOutputFormat. Report the execution time and the number of performed
comparisons.}

\subsubsection{run\_fast\_similarity.sh}
We wrapped the execution of the necessary jobs in the \textbf{run\_fast\_similarity.sh}.
For consistency with the previous method, we also only keep the first 5000 lines.
\lstinputlisting{assets/code/run_fast_similarity.sh}

\subsubsection{FastSimilarity.java}
\begin{itemize}
  \item \textbf{Mapper}:\\
  The mapper is based in the inverted index mapper from the previous assignment.
  We output a key value pair for the first words of each document with the key being
  the current word, and the value, the document it appears in.
  The first words to take into account is given by the cutoff index: $|d| - ceil(t \times |d|) + 1$.
  We don't need to index more words because if all those first words don't match,
  then it was shown that the similarity is below our threshold, hence we can skip
  the comparison.
  \lstinputlisting[firstnumber=82, firstline=82, lastline=99, language=java]{assets/code/FastSimilarity.java}

  \item \textbf{Reducer}:\\
  For each reduce call, the reducer will compare all documents for which the considered
  word appears.

  Some comparisons will be made multiple times for documents that have several words
  in common.
  However, having sorted the words by ascending order of frequency ensures that
  the most common words are not indexed, and that we make the minimum of comparisons possible.

  In order to skip the duplicated comparisons, we could just have kept track
  of the pairs that were already compared in a static variable.
  This could however defeat the purpose of distributed computing (difficult
  to have one shared static variable between all devices).
  \lstinputlisting[firstnumber=102, firstline=102, lastline=132, language=java]{assets/code/FastSimilarity.java}

  \item \textbf{Output}:\\
  Here is an extract of the most similar documents (the output file is cluttered
  with licence lines which are 100\% similar).
  \lstinputlisting[firstnumber=309, firstline=309, lastline=319]{assets/code/fastsimilarity.csv}
\end{itemize}

Our custom counter gives us a number of comparison is \textbf{107200}.
The execution time is \textbf{2.54 s}.
This is a huge improvement over the previous method.


\subsection{(10) Explain and justify the difference between a) and b) in the number of performed
comparisons, as well as their difference in execution time.}
\begin{itemize}
\item
In the first algorithm, we naively compared all documents with every other.
The computation time grows in $n^2$ which is not feasible for a large set of
documents as is common in big web companies.

\item
The second smarter method, starts from the idea that documents with very few
words in common shouldn't even be compared (this is the case for a majority of pairs).

The first idea is to index only the first  $|d| - ceil(t \times |d|) + 1$ which
ensures that documents with no words in common in those words will have a similarity
lower than the threshold.

A Second idea is that indexing the rare words before the more common ones will
also lead to less comparisons. These words will indeed appear in a smaller
subset of documents hence less comparisons.

\end{itemize}

The difference in execution time is directly linked to the number of comparisons
and the number of key value pairs that are passed between the mappers and reducers.


\section{Conclusion}
This approach can used for information retrieval with search engines for example.
The query being the document we want to compare to all others.
This could also be used for documents clustering, the jaccard similarity giving
us distances between documents.

In order to improve the relevance of the results we could use more advanced
similarity measures such as the well know TF-IDF measure.
\end{document}
