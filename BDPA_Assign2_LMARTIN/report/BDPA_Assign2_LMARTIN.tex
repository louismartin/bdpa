\documentclass[a4paper,10pt]{article}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}       % include graphics
\usepackage{subcaption}     % subfigures
\usepackage{float}          % placement of floats
\usepackage{fancyhdr}       % head notes and foot notes
\usepackage{bbm}            % Nice symbols
\usepackage{mathtools}      % Math tools like operators
\usepackage{listings}       % Write code in a listing
\usepackage[left=2cm,right=2cm,top=3cm,bottom=3cm]{geometry}

\graphicspath{ {assets/} }

% Operators
\DeclarePairedDelimiter\abs{\lvert}{\rvert} % abs
\DeclarePairedDelimiter\norm{\lVert}{\rVert} % norm
\DeclareMathOperator*{\argmax}{arg\,max} % argmax
\newcommand{\p}{\mathbbm{P}} % Big P for probabilties
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}  % Partial derivative

% To prevent the tilde from being printing above with lstlisting
\lstset{
    literate={~} {$\sim$}{1},
    showstringspaces=false,
    numbers=left,
    breaklines=true
}

% Head and foot notes
\pagestyle{fancy}
\fancyhf{}
\lhead{Louis MARTIN}
\rhead{Big Data Processing and Analytics: Assignment 2}
\rfoot{Page \thepage}


\title{Big Data Processing and Analytics: Assignment 2}
\author{Louis MARTIN\\
\href{mailto:louis.martin@student.ecp.fr}{\tt louis.martin@student.ecp.fr}}


\begin{document}
\maketitle

In this assignment we are going to compare documents using the Jaccard similarity.
For the sake of simplicity we are going to run our algorithms on the works of
William Shakespeare, each line being considered as a document.

\section{Setup}
This section is the same as for the first assignment.
\subsection{System specifications}

\begin{itemize}
    \item \textbf{Operating system}:\\
    Ubuntu 16.04 (Native)
    \item \textbf{System specifications}:\\
    Model: Dell Inspiron 17R 5720\\
    Processor: i5-3210M\\
    Cores: 2\\
    Threads: 4\\
    Ram: 8 GB\\
    Storage: 256GB SSD (MLC)
    \item \textbf{Java version}:\\
    openjdk version "1.8.0\_121"\\
    OpenJDK Runtime Environment (build 1.8.0\_121-8u121-b13-0ubuntu1.16.04.2-b13)\\
    OpenJDK 64-Bit Server VM (build 25.121-b13, mixed mode)
    \item \textbf{Haddop version}:\\
    Hadoop 2.7.3\\
    Subversion https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff\\
    Compiled by root on 2016-08-18T01:41Z\\
    Compiled with protoc 2.5.0\\
    From source with checksum 2e4ce5f957ea4db193bce3734ff29ff4\\
    This command was run using /usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.3.jar\\

\end{itemize}

Hadoop was installed using \href{https://www.digitalocean.com/community/tutorials/how-to-install-hadoop-in-stand-alone-mode-on-ubuntu-16-04}{this tutorial} and configured using \href{https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html}{this tutorial}.


\subsection{Configuration}
The configuration comes from the \href{https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html}{official documentation} for a single Hadoop node cluster.
The following configuration files allows Hadoop and YARN to run in a pseudo-distributed mode.
\begin{itemize}
    \item \textbf{core-site.xml:}
    \lstinputlisting[firstline=19, lastline=24, language=xml]{assets/hadoop_conf/core-site.xml}
    \item \textbf{hdfs-site.xml:}
    \lstinputlisting[firstline=19, lastline=24, language=xml]{assets/hadoop_conf/hdfs-site.xml}
    \item \textbf{mapred-site.xml:}
    \lstinputlisting[firstline=19, lastline=24, language=xml]{assets/hadoop_conf/mapred-site.xml}
    \item \textbf{yarn-site.xml:}
    \lstinputlisting[firstline=15, lastline=29, language=xml]{assets/hadoop_conf/yarn-site.xml}
    \item \textbf{Commands to set up HDFS and YARN:}
    \begin{lstlisting}[language=bash]
      # Format the filesystem
      hdfs namenode -format
      # Start HDFS
      start-dfs.sh

      # Create directories to execute MapReduce jobs
      hdfs dfs -mkdir /user
      hdfs dfs -mkdir /user/louis

      # Put the data in HDFS
      hdfs dfs -put ~/dev/bdpa/a1/data data

      # Start YARN Ressource manager
      start-yarn.sh
    \end{lstlisting}
\end{itemize}

\subsection{Workflow}
As I ran everythin from the command line interface, I created helper shell scripts
in order to facilitate the compilation and execution of the mapreduce jobs.
\begin{itemize}

\item \textbf{compile.sh}:\\
The $compile.sh$ script which takes as input parameter the name of the main
class of the $.java$ file to compile it into a $.jar$ file.
\\Usage: \lstinline{./compile.sh Preprocess}.
\lstinputlisting[language=bash]{assets/code/compile.sh}
\item \textbf{run.sh}:\\
The compilation and execution of the mapreduce job is then wrapped into this script.
It takes as input the main class and the input file or directory.
\\Usage: \lstinline{./run.sh Preprocess data/corpus}.
\lstinputlisting[language=bash]{assets/code/run.sh}


\end{itemize}

\section{Pre-processing the input (10)}

The following preprocessing is done with a mapreduce job in \textbf{Preprocess.java}.
Each line is read by a single map call, processed and the written to a file.
The key we used to uniquely identify a line is its byte offset, which is natively
provided in the map call.
The value is the processed line.

\subsection{(2) Remove all stopwords (you can use the stopwords file of your previous
assignment), special characters (keep only [a-z],[A-Z] and [0-9]) and keep each unique
word only once per line. Don’t keep empty lines.}
\subsubsection{WordCount.java}
In order to remove the stopwords we get the number of occurences of each word in
the corpus and store it in wordcount.csv.
We will reuse these wordcounts for sorting the words.
\begin{itemize}
  \item \textbf{WordCount.java} (inspired from the official documentation and the first assignment):
  \lstinputlisting[firstnumber=18, firstline=18, lastline=51, language=java]{assets/code/WordCount.java}
  \item Output \textbf{wordcount.csv}:\\
  Here is an extract from the output
  \lstinputlisting[firstnumber=1010, firstline=1010, lastline=1020]{assets/code/wordcount.csv}
\end{itemize}

\subsubsection{Preprocess.java}
We then preprocess the lines using the wordcounts.
The preprocessing is done by a single map task (no reduce taks).
\lstinputlisting[firstnumber=124, firstline=124, lastline=126]{assets/code/Preprocess.java}
\begin{itemize}
  \item \textbf{Clean the line}:\\
  We first clean the lines from characters other than alpha-numerical characters.
  \lstinputlisting[firstnumber=77, firstline=77, lastline=79]{assets/code/Preprocess.java}

  \item \textbf{Keep each word once per line}:\\
  \lstinputlisting[firstnumber=80, firstline=80, lastline=81]{assets/code/Preprocess.java}

  \item \textbf{Remove all stopwords}:\\
  A stopword is defined by a word with less than 4000 occurences.
  The wordcounts are stored in a hashmap for O(1) element retrieval.
  \lstinputlisting[firstnumber=82, firstline=82, lastline=87]{assets/code/Preprocess.java}
\end{itemize}

\subsection{(1) Store on HDFS the number of output records (i.e., total lines).}
As we implemented a map only job, the number of output records is given by the Counter
\textbf{MAP\_OUTPUT\_RECORDS}. The number of output records is \textbf{114982}.
\lstinputlisting[firstnumber=139, firstline=139, lastline=150]{assets/code/Preprocess.java}

\subsection{(7) Order the tokens of each line in ascending order of global frequency.}
We need to sort the resulting words based on their respective count.
With this aim in mind, we implemented a custom comparator to sort the array of words
based on their counts that we previously stored in a HashMap:
\lstinputlisting[firstnumber=67, firstline=67, lastline=73]{assets/code/Preprocess.java}
We then sort the words using this comparator:
\lstinputlisting[firstnumber=91, firstline=91, lastline=91]{assets/code/Preprocess.java}




\section{Set-similarity joins (90)}
\subsection{(40) Perform all pair-wise comparisons between documents, using the following
technique: Each document is handled by a single mapper (remember that lines are
used to represent documents in this assignment). The map method should emit, for
each document, the document id along with one other document id as a key (one such
pair for each other document in the corpus) and the document’s content as a value.
In the reduce phase, perform the Jaccard computations for all/some selected pairs.
Output only similar pairs on HDFS, in TextOutputFormat.
Make sure that the same pair of documents is compared no more than once. Report
the execution time and the number of performed comparisons.}

\subsection{(40) Create an inverted index, only for the first $|d| - ceil(t \times |d|) + 1$ words of each
document d (remember that they are stored in ascending order of frequency). In your
reducer, compute the similarity of the document pairs. Output only similar pairs on
HDFS, in TextOutputFormat. Report the execution time and the number of performed
comparisons.}

Each word is unique in each line -> second method no duplicates value pairs
\subsection{(10) Explain and justify the difference between a) and b) in the number of performed
comparisons, as well as their difference in execution time.}

\end{itemize}
\end{document}
